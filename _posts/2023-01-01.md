---
title: 'Conformal Inference'
date: 2023-1-1
permalink: /posts/2022/12/NewPost/
tags:
  - Conformal Inference
  - Prediction Interval
---

Decision making is built around uncertainty. You may be willing to undergo a surgical procedure if the doctor is confident in the diagnosis. If the doctor expresses more uncertainty, you may find the risk is not work taking and opt for other treatments. However, typically you will see output of predictive models as only a scalar value for regression or single class for a categorical problem. You might see the classification of an image as either "Cat" or "Dog". For binary classification, you can select a loss function to give a continuous value from 0-1 as a proxy for the confidence. However, there are limited options for multi-class classification and regression. Conformal inference has emerged as a method for quantifying uncertainty by giving a set or interval as a model output. In this post, I will describe why this is a promising application for statistical decision theory.

Imagine driving down the road during a thunderstorm. You see a sign in the distance but you can't tell what kind it is. The color is not distinct and the shape in not clear through the rain. You decide to slow down because of the uncertainty. As you approach, the red octagon becomes clear as do the other cars in the intersection. Fortunately, you used your uncertainty to guide your decision to be cautious so there was no accident.

Now, consider a self-driving car in those same conditions. The cameras on the vehicle do not have a clear view of the same stop sign and are unable to correctly classify it as such. If the algorithm does not predict that there is a stop sign then it will not know to stop. It might not have the same decision structure to know to slow down in uncertainty. If it is unable to slow down in time once the stop sign becomes clear, it may be too late. Fortunately, there are bright minds working to incorporate this uncertainty into the future of machine learning.

The main concept behind conformal inference is to deliver a prediction interval that contains the true value with a $1-\alpha$ probability. What is powerful about conformal inference is that there are no underlying assuptions about the distribution. You do not need to restrict this to 

Conformal inference originally was applicable to classification and regression problems but there are now extensions for <a href="http://proceedings.mlr.press/v139/xu21h.html?ref=https://codemonkey.link">time series data</a>, <a href="https://arxiv.org/pdf/2208.02814.pdf">segmentation</a>, <a href="https://arxiv.org/pdf/2208.02814.pdf"> data from changing distributions</a>, and <a href="https://arxiv.org/pdf/2104.08279.pdf">outlier detection</a>. It seems hard to see the end of the expansion of where conformal inference can be applied.

If you are interested in learning more, the following link has a great (and much more thorough) introduction to the topic complete with a history:
http://people.eecs.berkeley.edu/~angelopoulos/publications/downloads/gentle_intro_conformal_dfuq.pdf
